services:
  llamacpp:
    container_name: ${INFERENCE_ENG}
    image: gclub/llama.cpp:${INFERENCE_ENG_VERSION}
    restart: no
    deploy: # https://github.com/compose-spec/compose-spec/blob/master/deploy.md
      resources:
        reservations:
          cpus: "${NUM_CPU_CORES}"
    volumes:
      - "${DOCKER_VOLUME_DIRECTORY:-.}/volumes/models:/models"
    expose:
      - 8080
    ports:
      - 8080:8080
    command: ["-m", "models/${LANGUAGE_MODEL_NAME}","-c","8192"]

  embedding_eng:
    container_name: ${EMBEDDING_ENG}
    image: gclub/llama.cpp:${INFERENCE_ENG_VERSION}
    restart: no
    deploy: # https://github.com/compose-spec/compose-spec/blob/master/deploy.md
      resources:
        reservations:
          cpus: "${NUM_CPU_CORES_EMBEDDING}"
    volumes:
      - "${DOCKER_VOLUME_DIRECTORY:-.}/volumes/models:/models"
    expose:
      - 8080
    ports:
      - 8082:8080
    command: ["-m", "models/${EMBEDDING_MODEL_NAME}","--embeddings","--pooling","mean","-c","512"]
  
  voyager:
    container_name: voyager
    restart: no
    build:
      dockerfile: Dockerfile
      context: .
    volumes:
      - .:/app
    expose:
      - 8000
    ports:
      - 8000:8000
    depends_on:
      - llamacpp
      - embedding_eng